<div align="center">
 
# **Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition**  
 
 </div>

 ![](images/visual_abs.jpg)

 

---

## üìù Abstract
In this paper, we address the following question: How do generic foundation models (e.g., CLIP, BLIP, GPT-4o) compare against a domain-specific face recognition model (viz., AdaFace or ArcFace) on the face recognition task? Through a series of experiments involving several foundation models and benchmark datasets, we report the following findings: (a) In all face benchmark datasets considered, domain-specific models outperformed zero-shot foundation models. (b) The performance of zero-shot generic foundation models improved on over-segmented face images compared to tightly cropped faces, thereby suggesting the importance of contextual clues. (c) A simple score-level fusion of a foundation model with a domain-specific face recognition model improved the accuracy at low false match rates. (d) Foundation models, such as GPT-4o, are able to provide explainability to the face recognition pipeline. In some instances, foundation models are even able to resolve low-confidence decisions made by AdaFace,  thereby reiterating the importance of combining domain-specific face recognition models with generic foundation models in a judicious manner. 


---


![Image 1](images/1.jpg)
![Image 2](images/2.jpg)
![Image 3](images/3.jpg)
![Image 4](images/4.jpg)
![Image 5](images/5.jpg)
![Image 6](images/6.jpg)
![Image 7](images/7.jpg)
![Image 8](images/8.jpg)
![Ref-1](images/ref-1.jpg)
![Ref-2](images/ref-2.jpg)
![Ref-3](images/ref-3.jpg)
